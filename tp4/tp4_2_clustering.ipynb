{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering du corpus global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour le clustering\n",
    "import collections\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../tp4/data/data_all/\"\n",
    "\n",
    "files = [f for f in sorted(os.listdir(data_path)) if f\"_\" in f]\n",
    "texts = []\n",
    "for f in files:\n",
    "    try:\n",
    "        with open(data_path + f, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "            texts.append(text)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(data_path + f, \"r\", encoding=\"latin-1\") as file:\n",
    "            text = file.read()\n",
    "            texts.append(text)\n",
    "\n",
    "\n",
    "# Vectoriser les documents à l'aide de TF-IDF\n",
    "\n",
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=0.5,\n",
    "    min_df=0.1,\n",
    "    lowercase=True)\n",
    "\n",
    "\n",
    "# Construire la matrice de vecteurs à l'aide de la fonction `fit_transform`\n",
    "\n",
    "tfidf_vectors = vectorizer.fit_transform(texts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# déterminer le nombre de clusters optimal par essais-erreurs\n",
    "\n",
    "N_CLUSTERS = 2\n",
    "\n",
    "km_model = KMeans(n_clusters=N_CLUSTERS)\n",
    "\n",
    "clusters = km_model.fit_predict(tfidf_vectors)\n",
    "\n",
    "clustering = collections.defaultdict(list)\n",
    "\n",
    "for idx, label in enumerate(clusters):\n",
    "    clustering[label].append(files[idx])\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(tfidf_vectors.toarray())\n",
    "\n",
    "\n",
    "x_axis = reduced_vectors[:, 0]\n",
    "y_axis = reduced_vectors[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(x_axis, y_axis, s=100, c=clusters)\n",
    "\n",
    "# Ajouter les centroïdes\n",
    "centroids = pca.transform(km_model.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],  marker = \"x\", s=100, linewidths = 2, color='black')\n",
    "\n",
    "# Ajouter la légende\n",
    "legend_labels = ['Cluster 1', 'Cluster 2']\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=legend_labels)\n",
    "plt.title(\"Figure 3. Clustering du corpus complet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correspondance clusters - orientation politique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_right = \"../tp4/data/data_right/\"\n",
    "data_path_left = \"../tp4/data/data_left/\"\n",
    "\n",
    "# Charger les documents du dossier 'data_right'\n",
    "files_right = [f for f in sorted(os.listdir(data_path_right)) if f\"_\" in f]\n",
    "texts_right = []\n",
    "for f in files_right:\n",
    "    try:\n",
    "        with open(data_path_right + f, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "            texts_right.append(text)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(data_path_right + f, \"r\", encoding=\"latin-1\") as file:\n",
    "            text = file.read()\n",
    "            texts_right.append(text)\n",
    "\n",
    "# Charger les documents du dossier 'data_left'\n",
    "files_left = [f for f in sorted(os.listdir(data_path_left)) if f\"_\" in f]\n",
    "texts_left = []\n",
    "for f in files_left:\n",
    "    try:\n",
    "        with open(data_path_left + f, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "            texts_left.append(text)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(data_path_left + f, \"r\", encoding=\"latin-1\") as file:\n",
    "            text = file.read()\n",
    "            texts_left.append(text)\n",
    "\n",
    "# Vectoriser les documents à l'aide de TF-IDF\n",
    "\n",
    "# Création d'une fonction de pré-traitement\n",
    "def preprocessing(text, stem=True):\n",
    "    \"\"\" Tokenize text and remove punctuation \"\"\"\n",
    "    text = text.translate(string.punctuation)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Instancier le modèle TF-IDF avec ses arguments\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=preprocessing,\n",
    "    stop_words=stopwords.words('french'),\n",
    "    max_df=0.5,\n",
    "    min_df=0.1,\n",
    "    lowercase=True)\n",
    "\n",
    "# Vectoriser les documents de chaque dossier à l'aide de TF-IDF\n",
    "\n",
    "tfidf_vectors_right = vectorizer.fit_transform(texts_right)\n",
    "tfidf_vectors_left = vectorizer.fit_transform(texts_left)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser KMeans pour le clustering sur les documents de chaque dossier\n",
    "N_CLUSTERS = 1\n",
    "km_model_right = KMeans(n_clusters=N_CLUSTERS)\n",
    "km_model_left = KMeans(n_clusters=N_CLUSTERS)\n",
    "\n",
    "clusters_right = km_model_right.fit_predict(tfidf_vectors_right)\n",
    "clusters_left = km_model_left.fit_predict(tfidf_vectors_left)\n",
    "\n",
    "# Représentation graphique avec différenciation des dossiers\n",
    "# ...\n",
    "# Votre code pour la représentation graphique en utilisant les clusters_right et clusters_left pour distinguer les dossiers\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction de dimension pour la visualisation\n",
    "pca_right = PCA(n_components=2)\n",
    "reduced_vectors_right = pca_right.fit_transform(tfidf_vectors_right.toarray())\n",
    "\n",
    "pca_left = PCA(n_components=2)\n",
    "reduced_vectors_left = pca_left.fit_transform(tfidf_vectors_left.toarray())\n",
    "\n",
    "# Coordonnées des points pour chaque dossier\n",
    "x_axis_right = reduced_vectors_right[:, 0]\n",
    "y_axis_right = reduced_vectors_right[:, 1]\n",
    "\n",
    "x_axis_left = reduced_vectors_left[:, 0]\n",
    "y_axis_left = reduced_vectors_left[:, 1]\n",
    "\n",
    "# Création du graphique\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Dossiers 'data_right' et 'data_left' représentés par des couleurs différentes\n",
    "scatter_right = plt.scatter(\n",
    "    x_axis_right, y_axis_right, s=10, c='blue',\n",
    "    label='Presse de droite'\n",
    ")\n",
    "scatter_left = plt.scatter(\n",
    "    x_axis_left, y_axis_left, s=10, c='red',\n",
    "    label='Presse de gauche'\n",
    ")\n",
    "\n",
    "# Ajout des centroïdes pour chaque dossier\n",
    "centroids_right = pca_right.transform(km_model_right.cluster_centers_)\n",
    "plt.scatter(centroids_right[:, 0], centroids_right[:, 1], marker=\"x\", s=100, linewidths=2, color='black')\n",
    "\n",
    "centroids_left = pca_left.transform(km_model_left.cluster_centers_)\n",
    "plt.scatter(centroids_left[:, 0], centroids_left[:, 1], marker=\"x\", s=100, linewidths=2, color='black')\n",
    "\n",
    "# Ajout de la légende\n",
    "plt.legend(handles=[scatter_right, scatter_left], title=\"Orientation politique\")\n",
    "plt.title(\"Figure 4. Distribution des documents selon l'orientation politique\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv_tac': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a942b0119f0c2604d4302f32a2a6e790f63eb4c9b0c297be7a26bd56fa8e02c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
